# Specify the database library used to store metric data on disk.
# Can be whisper or ceres. If you are not familiar with ceres
# it is recommended that you stick with whisper as ceres does not
# have any documentation yet, and works quite differently than whisper.
DATABASE = whisper

# Directory in which to store metric data. This should be specific to the
# database library to avoid conflicts. So if you switch DATABASE
# to ceres, you'd switch this to /opt/graphite/storage/ceres/
LOCAL_DATA_DIR = /opt/graphite/storage/whisper/


##############################
# Database-specific settings #
##############################

[whisper]
# On some systems it is desirable for whisper to write synchronously.
# Set this option to True if you'd like to try this. Basically it will
# shift the onus of buffering writes from the kernel into carbon's cache.
AUTOFLUSH = False

# Enabling this option will cause Whisper to lock each Whisper file it writes
# to with an exclusive lock (LOCK_EX, see: man 2 flock). This is useful when
# multiple carbon daemons are writing to the same files
LOCK_WRITES = False

# By default new Whisper files are created pre-allocated with the data region
# filled with zeros to prevent fragmentation and speed up contiguous reads and
# writes (which are common). Enabling this option will cause Whisper to create
# the file sparsely instead. Enabling this option may allow a large increase of
# MAX_CREATES_PER_MINUTE but may have longer term performance implications
# depending on the underlying storage configuration.
SPARSE_CREATE = False

[ceres]
# Ceres nodes can have many slices and caching the right ones can improve
# performance dramatically. Note that there are many trade-offs to tinkering
# with this, and unless you are a ceres developer you *really* should not
# mess with this. Valid values are:
#    latest - only the most recent slice is cached
#       all - all slices are cached
#      none - slice caching is disabled
DEFAULT_SLICE_CACHING_BEHAVIOR = latest

# If a Ceres node accumulates too many slices, performance can suffer.
# This can be caused by intermittently reported data. To mitigate
# slice fragmentation there is a tolerance for how much space can be
# wasted within a slice file to avoid creating a new one. That tolerance
# level is determined by MAX_SLICE_GAP, which is the number of consecutive
# null datapoints allowed in a slice file.
# If you set this very low, you will waste less of the *tiny* bit disk space
# that this feature wastes, and you will be prone to performance problems
# caused by slice fragmentation, which can be pretty severe.
# If you set this really high, you will waste a bit more disk space (each
# null datapoint wastes 8 bytes, but keep in mind your filesystem's block
# size). If you suffer slice fragmentation issues, you should increase this or
# run the ceres-maintenance defrag plugin more often. However you should not
# set it to be huge because then if a large but allowed gap occurs it has to
# get filled in, which means instead of a simple 8-byte write to a new file we
# could end up doing an (8 * MAX_SLICE_GAP)-byte write to the latest slice.
MAX_SLICE_GAP = 80
